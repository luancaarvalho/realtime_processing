{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e19dd0",
   "metadata": {},
   "source": [
    "# ðŸš• NYC Taxi Streaming Demo - Databricks Free Edition\n",
    "\n",
    "âœ¨ **CompatÃ­vel com Databricks Free Edition (SEM DBFS)**\n",
    "\n",
    "Este notebook demonstra **Structured Streaming** usando o dataset pÃºblico **NYC Taxi** disponÃ­vel nativamente no Databricks.\n",
    "\n",
    "## ðŸŽ¯ Objetivos da DemonstraÃ§Ã£o\n",
    "- Processar dados reais de tÃ¡xis de NYC com streaming\n",
    "- Usar **maxFilesPerTrigger=1** para visualizar processamento gradual em batches\n",
    "- Demonstrar agregaÃ§Ãµes em tempo real (viagens por regiÃ£o, receita por tipo de pagamento)\n",
    "- Salvar resultados diretamente em **Delta Tables** (sem DBFS)\n",
    "- Visualizar mÃ©tricas completas para ensino\n",
    "\n",
    "## ðŸ“Š Dataset: samples.nyctaxi.trips\n",
    "- **Fonte:** Unity Catalog do Databricks (jÃ¡ disponÃ­vel!)\n",
    "- **Registros:** MilhÃµes de viagens de tÃ¡xi em NYC\n",
    "- **PerÃ­odo:** Dados histÃ³ricos de tÃ¡xis amarelos e verdes\n",
    "- **Campos:** pickup/dropoff datetime, locations, distances, fares, payment types\n",
    "\n",
    "## âœ… Vantagens desta abordagem\n",
    "- âŒ Sem Kafka externo (Free Edition tem rede restrita)\n",
    "- âŒ Sem DBFS (Public DBFS root estÃ¡ desabilitado)\n",
    "- âœ… Usa dados REAIS (nÃ£o sintÃ©ticos)\n",
    "- âœ… 100% nativo do Databricks com Delta Tables\n",
    "- âœ… Zero configuraÃ§Ã£o de rede ou filesystem\n",
    "- âœ… Perfeito para demonstraÃ§Ãµes educacionais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541e1fb",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup e ConfiguraÃ§Ã£o (SEM DBFS)\n",
    "\n",
    "Vamos configurar o streaming **diretamente** da tabela Unity Catalog, sem usar DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ed258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# ConfiguraÃ§Ãµes do streaming\n",
    "TABLE_SOURCE = \"samples.nyctaxi.trips\"\n",
    "OUTPUT_TABLE = \"default.nyctaxi_streaming_results\"  # Tabela Delta gerenciada\n",
    "CHECKPOINT_LOCATION = \"/tmp/nyctaxi_checkpoint\"  # Checkpoint temporÃ¡rio (nÃ£o precisa DBFS pÃºblico)\n",
    "\n",
    "print(\"ðŸš• NYC TAXI STREAMING DEMO (SEM DBFS)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ðŸ“Š Fonte:      {TABLE_SOURCE} (Unity Catalog)\")\n",
    "print(f\"ðŸ’¾ Destino:    {OUTPUT_TABLE} (Delta Table)\")\n",
    "print(f\"ðŸ”– Checkpoint: {CHECKPOINT_LOCATION}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Abordagem:\")print(\"=\" * 80)\n",
    "\n",
    "print(\"   â€¢ Ler diretamente de Unity Catalog com readStream\")print(\"   â€¢ Visualizar mÃ©tricas com foreachBatch\")\n",
    "\n",
    "print(\"   â€¢ Processar batches com maxFilesPerTrigger\")print(\"   â€¢ Salvar em Delta Table gerenciada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41779415",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Explorar Dados da Tabela NYC Taxi\n",
    "\n",
    "Vamos primeiro entender os dados disponÃ­veis na tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Explorando samples.nyctaxi.trips...\")\n",
    "\n",
    "# Ler amostra da tabela NYC Taxi\n",
    "nyctaxi_sample = spark.table(TABLE_SOURCE).limit(10)\n",
    "\n",
    "print(\"\\nðŸ“‹ Schema dos dados:\")\n",
    "nyctaxi_sample.printSchema()\n",
    "\n",
    "print(\"\\nðŸ” Amostra dos primeiros registros:\")\n",
    "nyctaxi_sample.show(5, truncate=False)\n",
    "\n",
    "# Contar total disponÃ­vel (pode demorar)\n",
    "\n",
    "try:    print(\"\\nâš ï¸  Contagem total desabilitada para acelerar execuÃ§Ã£o\")\n",
    "\n",
    "    total_count = spark.table(TABLE_SOURCE).count()except:\n",
    "    print(f\"\\nðŸ“ˆ Total de registros disponÃ­veis: {total_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c8c4c",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Configurar Structured Streaming com maxFilesPerTrigger=1\n",
    "\n",
    "Agora criamos o streaming que processa **1 arquivo por vez** para demonstraÃ§Ã£o visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd739e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema do NYC Taxi (baseado em samples.nyctaxi.trips)\n",
    "taxi_schema = StructType([\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "    StructField(\"pickup_location_id\", IntegerType(), True),\n",
    "    StructField(\"dropoff_location_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Criar streaming DataFrame com maxFilesPerTrigger=1\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(taxi_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .parquet(INPUT_PATH)\n",
    "\n",
    "print(\"âœ… Streaming configurado!\")\n",
    "print(f\"âš™ï¸  maxFilesPerTrigger: 1 (processa 1 arquivo por vez)\")\n",
    "print(f\"ðŸ“Š Ã‰ streaming? {streaming_df.isStreaming}\")\n",
    "print(\"\\nðŸ“‹ Schema:\")\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03318cf4",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Enriquecer e Agregar Dados\n",
    "\n",
    "Adicionamos colunas de anÃ¡lise e criamos agregaÃ§Ãµes por regiÃ£o e tipo de pagamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enriquecer dados\n",
    "enriched_df = streaming_df \\\n",
    "    .withColumn(\"processing_time\", current_timestamp()) \\\n",
    "    .withColumn(\"file_name\", input_file_name()) \\\n",
    "    .withColumn(\"pickup_date\", to_date(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"trip_duration_minutes\", \n",
    "                (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60) \\\n",
    "    .withColumn(\"payment_type_desc\", \n",
    "                when(col(\"payment_type\") == 1, \"Credit Card\")\n",
    "                .when(col(\"payment_type\") == 2, \"Cash\")\n",
    "                .when(col(\"payment_type\") == 3, \"No Charge\")\n",
    "                .when(col(\"payment_type\") == 4, \"Dispute\")\n",
    "                .otherwise(\"Unknown\"))\n",
    "\n",
    "# AgregaÃ§Ãµes por localizaÃ§Ã£o de pickup e tipo de pagamento\n",
    "aggregated_df = enriched_df \\\n",
    "    .groupBy(\"pickup_location_id\", \"payment_type_desc\", \"pickup_date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_trips\"),\n",
    "        sum(\"trip_distance\").alias(\"total_distance\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"total_amount\").alias(\"avg_fare\"),\n",
    "        avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "        avg(\"trip_duration_minutes\").alias(\"avg_duration_min\"),\n",
    "        min(\"tpep_pickup_datetime\").alias(\"first_trip\"),\n",
    "        max(\"tpep_pickup_datetime\").alias(\"last_trip\")\n",
    "    )\n",
    "\n",
    "print(\"âœ… Pipeline de agregaÃ§Ã£o criado!\")\n",
    "print(\"\\nðŸ“Š MÃ©tricas calculadas:\")\n",
    "print(\"  â€¢ Total de viagens\")\n",
    "print(\"  â€¢ DistÃ¢ncia total percorrida\")\n",
    "print(\"  â€¢ Receita total\")\n",
    "print(\"  â€¢ Ticket mÃ©dio (fare)\")\n",
    "print(\"  â€¢ DistÃ¢ncia mÃ©dia\")\n",
    "print(\"  â€¢ DuraÃ§Ã£o mÃ©dia (minutos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea908e2",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ foreachBatch - Processar e Visualizar Cada Arquivo\n",
    "\n",
    "Usamos `foreachBatch` para ver o processamento **arquivo por arquivo** em tempo real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d891ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstatÃ­sticas acumuladas\n",
    "batch_stats = {\n",
    "    'count': 0,\n",
    "    'total_trips': 0,\n",
    "    'total_revenue': 0,\n",
    "    'total_distance': 0\n",
    "}\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"Processa cada batch (arquivo) do streaming\"\"\"\n",
    "    batch_stats['count'] += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(f\"ðŸ“¦ BATCH #{batch_stats['count']} | Batch ID: {batch_id}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    if batch_df.count() == 0:\n",
    "        print(\"âš ï¸  Batch vazio - pulando\")\n",
    "        return\n",
    "    \n",
    "    # EstatÃ­sticas do batch\n",
    "    stats = batch_df.agg(\n",
    "        sum(\"total_trips\").alias(\"trips\"),\n",
    "        sum(\"total_revenue\").alias(\"revenue\"),\n",
    "        sum(\"total_distance\").alias(\"distance\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    batch_stats['total_trips'] += stats.trips if stats.trips else 0\n",
    "    batch_stats['total_revenue'] += stats.revenue if stats.revenue else 0\n",
    "    batch_stats['total_distance'] += stats.distance if stats.distance else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š ESTATÃSTICAS DO BATCH:\")\n",
    "    print(f\"   ðŸš• Viagens: {stats.trips:,}\")\n",
    "    print(f\"   ðŸ’° Receita: ${stats.revenue:,.2f}\")\n",
    "    print(f\"   ðŸ“ DistÃ¢ncia: {stats.distance:,.2f} milhas\")\n",
    "    \n",
    "    # Top 5 localizaÃ§Ãµes por receita\n",
    "    print(f\"\\nðŸ† TOP 5 LOCALIZAÃ‡Ã•ES (Receita):\")\n",
    "    top_locations = batch_df.orderBy(col(\"total_revenue\").desc()).limit(5)\n",
    "    top_locations.select(\n",
    "        \"pickup_location_id\",\n",
    "        \"total_trips\",\n",
    "        \"total_revenue\",\n",
    "        \"avg_fare\"\n",
    "    ).show(truncate=False)\n",
    "    \n",
    "    # DistribuiÃ§Ã£o por tipo de pagamento\n",
    "    print(f\"ðŸ’³ DISTRIBUIÃ‡ÃƒO POR TIPO DE PAGAMENTO:\")\n",
    "    payment_dist = batch_df.groupBy(\"payment_type_desc\") \\\n",
    "        .agg(\n",
    "            sum(\"total_trips\").alias(\"trips\"),\n",
    "            sum(\"total_revenue\").alias(\"revenue\")\n",
    "        ) \\\n",
    "        .orderBy(col(\"revenue\").desc())\n",
    "    payment_dist.show(truncate=False)\n",
    "    \n",
    "    # Totais acumulados\n",
    "    print(f\"\\nðŸ“ˆ TOTAIS ACUMULADOS:\")\n",
    "    print(f\"   ðŸ“¦ Batches processados: {batch_stats['count']}\")\n",
    "    print(f\"   ðŸš• Viagens: {batch_stats['total_trips']:,}\")\n",
    "    print(f\"   ðŸ’° Receita: ${batch_stats['total_revenue']:,.2f}\")\n",
    "    print(f\"   ðŸ“ DistÃ¢ncia: {batch_stats['total_distance']:,.2f} milhas\")\n",
    "    print(f\"   ðŸŽ¯ Receita MÃ©dia/Viagem: ${batch_stats['total_revenue']/max(batch_stats['total_trips'],1):,.2f}\")\n",
    "\n",
    "print(\"ðŸ”§ FunÃ§Ã£o de processamento definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c05319",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ EXECUTAR Streaming com foreachBatch\n",
    "\n",
    "**Execute esta cÃ©lula e observe o processamento arquivo por arquivo!**\n",
    "\n",
    "â±ï¸ Aguarde ~60 segundos para processar todos os 10 arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetar estatÃ­sticas\n",
    "batch_stats = {'count': 0, 'total_trips': 0, 'total_revenue': 0, 'total_distance': 0}\n",
    "\n",
    "# Limpar checkpoint anterior\n",
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"ðŸš€ INICIANDO STRUCTURED STREAMING\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"ðŸ“‚ Source: {INPUT_PATH}\")\n",
    "print(f\"âš™ï¸  maxFilesPerTrigger: 1\")\n",
    "print(f\"â±ï¸  Trigger: a cada 5 segundos\")\n",
    "print(f\"ðŸ”– Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Iniciar streaming com foreachBatch\n",
    "query = aggregated_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(f\"\\nâœ… Stream ativo! Query ID: {query.id}\")\n",
    "print(\"â³ Processando... Observe os batches abaixo:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab057f67",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Parar o Streaming\n",
    "\n",
    "**Aguarde ~60 segundos, entÃ£o execute esta cÃ©lula para parar e ver o resumo final.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fe172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"â³ Aguardando 60 segundos para processar todos os arquivos...\")\n",
    "time.sleep(60)\n",
    "\n",
    "query.stop()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸ›‘ STREAMING FINALIZADO\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nðŸ“Š RESUMO FINAL:\")\n",
    "print(f\"   ðŸ“¦ Total de batches: {batch_stats['count']}\")\n",
    "print(f\"   ðŸš• Total de viagens: {batch_stats['total_trips']:,}\")\n",
    "print(f\"   ðŸ’° Receita total: ${batch_stats['total_revenue']:,.2f}\")\n",
    "print(f\"   ðŸ“ DistÃ¢ncia total: {batch_stats['total_distance']:,.2f} milhas\")\n",
    "print(f\"   ðŸŽ¯ Receita mÃ©dia/viagem: ${batch_stats['total_revenue']/max(batch_stats['total_trips'],1):,.2f}\")\n",
    "print(f\"   ðŸ“Š DistÃ¢ncia mÃ©dia/viagem: {batch_stats['total_distance']/max(batch_stats['total_trips'],1):.2f} milhas\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ad988",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ OPÃ‡ÃƒO 2: Gravar em Delta Lake\n",
    "\n",
    "Alternativa ao foreachBatch: gravar diretamente em Delta Lake para consultas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar checkpoint anterior\n",
    "try:\n",
    "    dbutils.fs.rm(f\"{CHECKPOINT_PATH}_delta\", True)\n",
    "    dbutils.fs.rm(OUTPUT_PATH, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"ðŸš€ Iniciando streaming com saÃ­da para Delta Lake...\")\n",
    "print(f\"ðŸ’¾ Tabela: {OUTPUT_PATH}\")\n",
    "print(f\"ðŸ”– Checkpoint: {CHECKPOINT_PATH}_delta\\n\")\n",
    "\n",
    "# Streaming para Delta com agregaÃ§Ãµes\n",
    "query_delta = aggregated_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}_delta\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start(OUTPUT_PATH)\n",
    "\n",
    "print(f\"âœ… Stream Delta ativo! Query ID: {query_delta.id}\")\n",
    "print(f\"ðŸ’¡ Execute a prÃ³xima cÃ©lula apÃ³s ~60 segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22695635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "query_delta.stop()\n",
    "print(\"âœ… Stream Delta finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e9766",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Consultar Tabela Delta\n",
    "\n",
    "AnÃ¡lise batch dos dados processados pelo streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34973767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler tabela Delta\n",
    "delta_df = spark.read.format(\"delta\").load(OUTPUT_PATH)\n",
    "\n",
    "print(f\"ðŸ“Š Tabela Delta carregada!\")\n",
    "print(f\"ðŸ“ˆ Total de registros: {delta_df.count():,}\\n\")\n",
    "\n",
    "print(\"ðŸ† TOP 10 - LocalizaÃ§Ãµes + Tipo Pagamento (por Receita):\")\n",
    "delta_df.orderBy(col(\"total_revenue\").desc()).limit(10).show(truncate=False)\n",
    "\n",
    "# Totais gerais\n",
    "totals = delta_df.agg(\n",
    "    sum(\"total_trips\").alias(\"trips\"),\n",
    "    sum(\"total_revenue\").alias(\"revenue\"),\n",
    "    sum(\"total_distance\").alias(\"distance\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nðŸ’° TOTAIS GERAIS:\")\n",
    "print(f\"   ðŸš• Viagens: {totals.trips:,}\")\n",
    "print(f\"   ðŸ’µ Receita: ${totals.revenue:,.2f}\")\n",
    "print(f\"   ðŸ“ DistÃ¢ncia: {totals.distance:,.2f} milhas\")\n",
    "print(f\"   ðŸŽ¯ Ticket MÃ©dio: ${totals.revenue/totals.trips:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545e17b",
   "metadata": {},
   "source": [
    "## ðŸ”Ÿ Criar Tabela Gerenciada no Catalog (Opcional)\n",
    "\n",
    "Cria uma tabela gerenciada para consultas SQL facilitadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS nyctaxi_demo\")\n",
    "\n",
    "# Criar tabela gerenciada\n",
    "delta_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"nyctaxi_demo.trip_aggregations\")\n",
    "\n",
    "print(\"âœ… Tabela criada: nyctaxi_demo.trip_aggregations\")\n",
    "print(\"\\nðŸ“Š Amostra da tabela:\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM nyctaxi_demo.trip_aggregations LIMIT 10\").show(truncate=False)\n",
    "\n",
    "print(\"\\nðŸ’¡ Agora vocÃª pode consultar com SQL:\")\n",
    "print(\"   SELECT payment_type_desc, SUM(total_revenue)\")\n",
    "print(\"   FROM nyctaxi_demo.trip_aggregations\")\n",
    "print(\"   GROUP BY payment_type_desc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc0ddf",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Limpeza de Recursos\n",
    "\n",
    "Remove todos os arquivos temporÃ¡rios criados nesta demonstraÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§¹ Limpando recursos...\\n\")\n",
    "\n",
    "# Parar todos os streams ativos\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "    print(f\"âœ“ Stream {s.id} parado\")\n",
    "\n",
    "# Remover arquivos\n",
    "paths = [BASE_PATH]\n",
    "for path in paths:\n",
    "    try:\n",
    "        dbutils.fs.rm(path, True)\n",
    "        print(f\"âœ“ Removido: {path}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nâœ… Limpeza concluÃ­da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b49c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Conceitos Demonstrados\n",
    "\n",
    "### Structured Streaming\n",
    "- **maxFilesPerTrigger=1**: Processa arquivos um por vez (ideal para demonstraÃ§Ã£o visual)\n",
    "- **foreachBatch**: Controle total sobre cada micro-batch processado\n",
    "- **Delta Lake Output**: GravaÃ§Ã£o incremental com ACID transactions\n",
    "- **AgregaÃ§Ãµes em Streaming**: GROUP BY com mÃºltiplas mÃ©tricas (count, sum, avg)\n",
    "\n",
    "### Checkpoint & Fault Tolerance\n",
    "- Checkpoints garantem processamento exactly-once\n",
    "- RecuperaÃ§Ã£o automÃ¡tica em caso de falhas\n",
    "- Estado da agregaÃ§Ã£o mantido entre micro-batches\n",
    "\n",
    "### Performance\n",
    "- Particionamento dos dados de entrada para paralelizaÃ§Ã£o\n",
    "- AgregaÃ§Ãµes otimizadas com Catalyst optimizer\n",
    "- Delta Lake para leitura/escrita eficiente\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Para Seus Alunos\n",
    "\n",
    "**Conceitos-chave demonstrados:**\n",
    "\n",
    "1. **Processamento Incremental**: Cada arquivo Ã© um micro-batch\n",
    "2. **AgregaÃ§Ãµes Stateful**: MantÃ©m estado acumulado (complete mode)\n",
    "3. **Observabilidade**: MÃ©tricas detalhadas batch por batch\n",
    "4. **Delta Lake**: Camada de armazenamento confiÃ¡vel para streaming\n",
    "5. **Dados Reais**: NYC Taxi demonstra aplicaÃ§Ãµes prÃ¡ticas\n",
    "\n",
    "**PrÃ³ximos passos:**\n",
    "1. Modificar agregaÃ§Ãµes (diferentes GROUP BY)\n",
    "2. Adicionar filtros (apenas viagens > 5 milhas)\n",
    "3. Window functions (agregaÃ§Ãµes por janela de tempo)\n",
    "4. Joins com dados estÃ¡ticos (ex: informaÃ§Ãµes de zonas)\n",
    "5. Integrar com fontes reais (quando sair do Free Edition)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Recursos Adicionais\n",
    "\n",
    "**DocumentaÃ§Ã£o Oficial:**\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Databricks Structured Streaming](https://docs.databricks.com/structured-streaming/index.html)\n",
    "- [Delta Lake Streaming](https://docs.delta.io/latest/delta-streaming.html)\n",
    "- [Unity Catalog Datasets](https://docs.databricks.com/discover/databricks-datasets.html)\n",
    "\n",
    "**Databricks Free Edition:**\n",
    "- Use `display()` para visualizaÃ§Ãµes interativas\n",
    "- Workspace â†’ Data â†’ Catalog Explorer para navegar tabelas\n",
    "- Monitor: Workspace â†’ Compute â†’ seu cluster â†’ Streaming Queries\n",
    "\n",
    "**IntegraÃ§Ã£o com Git:**\n",
    "1. Workspace â†’ Git Folders â†’ Add Repo\n",
    "2. Clone: `https://github.com/seu-usuario/seu-repo`\n",
    "3. Commits/Push direto da UI do Databricks\n",
    "4. Branches: Use Git Folders para alternar branches\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Checklist de ExecuÃ§Ã£o\n",
    "\n",
    "- [ ] CÃ©lula 2: Ler dados de `samples.nyctaxi.trips`\n",
    "- [ ] CÃ©lula 3: Particionar em 10 arquivos Parquet\n",
    "- [ ] CÃ©lula 4: Configurar streaming com maxFilesPerTrigger=1\n",
    "- [ ] CÃ©lula 5: Definir agregaÃ§Ãµes\n",
    "- [ ] CÃ©lula 6: Definir funÃ§Ã£o foreachBatch\n",
    "- [ ] CÃ©lula 7: **EXECUTAR streaming** (observe os batches!)\n",
    "- [ ] CÃ©lula 8: Parar e ver resumo final\n",
    "- [ ] CÃ©lula 9-11: (Opcional) Gravar em Delta Lake\n",
    "- [ ] CÃ©lula 12: (Opcional) Consultar resultados\n",
    "- [ ] CÃ©lula 13: (Opcional) Criar tabela gerenciada\n",
    "- [ ] CÃ©lula 14: Limpar recursos\n",
    "\n",
    "**ðŸŽ¯ Perfeito para ensinar os fundamentos de Structured Streaming com dados REAIS!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
