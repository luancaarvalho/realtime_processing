{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482d288e",
   "metadata": {},
   "source": [
    "# üìä Demo Completo: Structured Streaming com Parquet\n",
    "\n",
    "Notebook completo para demonstrar **Structured Streaming** processando arquivos Parquet arquivo por arquivo.\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Processar arquivos um por um com `maxFilesPerTrigger=1`\n",
    "- Gerar dados sint√©ticos de e-commerce (8 arquivos Parquet)\n",
    "- Demonstrar processamento vis√≠vel batch por batch\n",
    "- Criar tabelas Delta Lake automaticamente\n",
    "- Agrega√ß√µes em tempo real por categoria e regi√£o\n",
    "- Visualizar m√©tricas completas de processamento\n",
    "\n",
    "## ‚úÖ Compat√≠vel com Databricks Free Edition\n",
    "- Sem depend√™ncias externas (Kafka, etc)\n",
    "- Usa dados sint√©ticos gerados pelo pr√≥prio notebook\n",
    "- Funciona em Serverless Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b52b456",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Detectar ambiente\n",
    "IS_DATABRICKS = 'DATABRICKS_RUNTIME_VERSION' in os.environ\n",
    "\n",
    "# Configurar paths\n",
    "if IS_DATABRICKS:\n",
    "    BASE_PATH = '/FileStore/streaming_demo'\n",
    "else:\n",
    "    BASE_PATH = '/tmp/streaming_demo'\n",
    "\n",
    "INPUT_PATH = f'{BASE_PATH}/input'\n",
    "OUTPUT_PATH = f'{BASE_PATH}/output'\n",
    "CHECKPOINT_PATH = f'{BASE_PATH}/checkpoint'\n",
    "\n",
    "print(f'üîß Ambiente: {\"Databricks\" if IS_DATABRICKS else \"Local\"}')\n",
    "print(f'üìÇ Input: {INPUT_PATH}')\n",
    "print(f'üíæ Output: {OUTPUT_PATH}')\n",
    "print(f'üîñ Checkpoint: {CHECKPOINT_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091c52e",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Gerar Dados Sint√©ticos de E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997628df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema dos dados\n",
    "schema = StructType([\n",
    "    StructField('transaction_id', IntegerType(), False),\n",
    "    StructField('customer_id', IntegerType(), False),\n",
    "    StructField('product', StringType(), False),\n",
    "    StructField('category', StringType(), False),\n",
    "    StructField('quantity', IntegerType(), False),\n",
    "    StructField('price', DoubleType(), False),\n",
    "    StructField('total', DoubleType(), False),\n",
    "    StructField('timestamp', TimestampType(), False),\n",
    "    StructField('region', StringType(), False),\n",
    "    StructField('payment_method', StringType(), False)\n",
    "])\n",
    "\n",
    "def generate_transactions(n=500, batch_id=0):\n",
    "    \"\"\"Gera transa√ß√µes sint√©ticas de e-commerce\"\"\"\n",
    "    categories = ['Electronics', 'Books', 'Clothing', 'Food', 'Sports', 'Home']\n",
    "    products = {\n",
    "        'Electronics': ['Laptop', 'Phone', 'Tablet', 'Headphones'],\n",
    "        'Books': ['Fiction', 'Tech', 'Business', 'Science'],\n",
    "        'Clothing': ['Shirt', 'Pants', 'Shoes', 'Jacket'],\n",
    "        'Food': ['Snacks', 'Drinks', 'Frozen', 'Fresh'],\n",
    "        'Sports': ['Equipment', 'Apparel', 'Accessories'],\n",
    "        'Home': ['Furniture', 'Decor', 'Kitchen', 'Garden']\n",
    "    }\n",
    "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "    payments = ['Credit Card', 'Debit Card', 'Cash', 'Digital Wallet']\n",
    "    \n",
    "    data = []\n",
    "    base_id = batch_id * n\n",
    "    base_time = datetime.now() - timedelta(hours=24)\n",
    "    \n",
    "    for i in range(n):\n",
    "        category = random.choice(categories)\n",
    "        product = random.choice(products[category])\n",
    "        qty = random.randint(1, 5)\n",
    "        price = round(random.uniform(10, 500), 2)\n",
    "        \n",
    "        data.append((\n",
    "            base_id + i,\n",
    "            random.randint(1000, 9999),\n",
    "            product,\n",
    "            category,\n",
    "            qty,\n",
    "            price,\n",
    "            round(qty * price, 2),\n",
    "            base_time + timedelta(minutes=random.randint(0, 1440)),\n",
    "            random.choice(regions),\n",
    "            random.choice(payments)\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "print('‚úÖ Fun√ß√£o de gera√ß√£o definida!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc428bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üé≤ Gerando 8 arquivos Parquet para demonstra√ß√£o...\\n')\n",
    "\n",
    "# Limpar pasta de input\n",
    "if IS_DATABRICKS:\n",
    "    try:\n",
    "        dbutils.fs.rm(INPUT_PATH, True)\n",
    "    except:\n",
    "        pass\n",
    "    dbutils.fs.mkdirs(INPUT_PATH)\n",
    "\n",
    "# Gerar 8 arquivos\n",
    "for i in range(8):\n",
    "    data = generate_transactions(n=300, batch_id=i)\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    \n",
    "    file_path = f'{INPUT_PATH}/transactions_batch_{i:03d}.parquet'\n",
    "    df.coalesce(1).write.mode('overwrite').parquet(file_path)\n",
    "    \n",
    "    print(f'  ‚úÖ Arquivo {i+1}/8: transactions_batch_{i:03d}.parquet ({len(data)} transa√ß√µes)')\n",
    "\n",
    "print(f'\\n‚úÖ Total: {8 * 300} transa√ß√µes em 8 arquivos')\n",
    "print(f'üìÇ Salvos em: {INPUT_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ad3e8",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configurar Structured Streaming com maxFilesPerTrigger=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar streaming DataFrame com maxFilesPerTrigger=1\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .option('maxFilesPerTrigger', 1) \\\n",
    "    .parquet(INPUT_PATH)\n",
    "\n",
    "# Adicionar colunas de controle\n",
    "enriched_df = streaming_df \\\n",
    "    .withColumn('processing_time', current_timestamp()) \\\n",
    "    .withColumn('file_name', input_file_name()) \\\n",
    "    .withColumn('date', to_date(col('timestamp'))) \\\n",
    "    .withColumn('hour', hour(col('timestamp'))) \\\n",
    "    .withColumn('revenue', round(col('total'), 2))\n",
    "\n",
    "print('‚úÖ Streaming DataFrame configurado!')\n",
    "print(f'\\nüìä Schema:')\n",
    "enriched_df.printSchema()\n",
    "print(f'\\nüîÑ √â streaming? {streaming_df.isStreaming}')\n",
    "print(f'‚öôÔ∏è  maxFilesPerTrigger: 1 (processa 1 arquivo por vez)')\n",
    "print(f'üìù Colunas adicionadas: processing_time, file_name, date, hour, revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c2854",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Criar Agrega√ß√µes por Categoria e Regi√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega√ß√£o por categoria e regi√£o\n",
    "aggregated_df = enriched_df \\\n",
    "    .groupBy('category', 'region', 'date') \\\n",
    "    .agg(\n",
    "        count('*').alias('total_transactions'),\n",
    "        sum('quantity').alias('total_items'),\n",
    "        sum('revenue').alias('total_revenue'),\n",
    "        avg('revenue').alias('avg_transaction_value'),\n",
    "        countDistinct('customer_id').alias('unique_customers'),\n",
    "        min('timestamp').alias('first_transaction'),\n",
    "        max('timestamp').alias('last_transaction')\n",
    "    ) \\\n",
    "    .withColumn('avg_transaction_value', round(col('avg_transaction_value'), 2)) \\\n",
    "    .withColumn('total_revenue', round(col('total_revenue'), 2))\n",
    "\n",
    "print('‚úÖ Pipeline de agrega√ß√£o criado!')\n",
    "print('üìä M√©tricas calculadas:')\n",
    "print('   ‚Ä¢ Total de transa√ß√µes')\n",
    "print('   ‚Ä¢ Total de itens vendidos')\n",
    "print('   ‚Ä¢ Receita total')\n",
    "print('   ‚Ä¢ Valor m√©dio por transa√ß√£o')\n",
    "print('   ‚Ä¢ Clientes √∫nicos')\n",
    "print('   ‚Ä¢ Primeira e √∫ltima transa√ß√£o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d8b2f",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Definir Fun√ß√£o de Processamento Batch (foreachBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas globais\n",
    "batch_stats = {'count': 0, 'total_records': 0, 'total_revenue': 0}\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"Processa cada batch mostrando m√©tricas detalhadas\"\"\"\n",
    "    batch_stats['count'] += 1\n",
    "    \n",
    "    print('\\n' + '='*100)\n",
    "    print(f'üì¶ BATCH #{batch_stats[\"count\"]} | Batch ID: {batch_id}')\n",
    "    print('='*100)\n",
    "    \n",
    "    num_records = batch_df.count()\n",
    "    \n",
    "    if num_records == 0:\n",
    "        print('‚ö†Ô∏è  Batch vazio - aguardando novos arquivos...')\n",
    "        return\n",
    "    \n",
    "    batch_stats['total_records'] += num_records\n",
    "    \n",
    "    # Informa√ß√µes do arquivo processado\n",
    "    files = batch_df.select('file_name').distinct().collect()\n",
    "    if files:\n",
    "        print(f'\\nüìÑ Arquivo(s) processado(s):')\n",
    "        for f in files:\n",
    "            fname = f.file_name.split('/')[-1] if f.file_name else 'unknown'\n",
    "            print(f'   ‚Ä¢ {fname}')\n",
    "    \n",
    "    # Estat√≠sticas do batch\n",
    "    stats = batch_df.agg(\n",
    "        count('*').alias('transactions'),\n",
    "        sum('revenue').alias('revenue'),\n",
    "        countDistinct('customer_id').alias('customers'),\n",
    "        avg('revenue').alias('avg_ticket')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    batch_stats['total_revenue'] += stats.revenue if stats.revenue else 0\n",
    "    \n",
    "    print(f'\\nüìä ESTAT√çSTICAS DO BATCH:')\n",
    "    print(f'   üí≥ Transa√ß√µes: {stats.transactions:,}')\n",
    "    print(f'   üí∞ Receita: ${stats.revenue:,.2f}')\n",
    "    print(f'   üë• Clientes: {stats.customers:,}')\n",
    "    print(f'   üéØ Ticket M√©dio: ${stats.avg_ticket:,.2f}')\n",
    "    \n",
    "    # Top categorias\n",
    "    print(f'\\nüèÜ TOP 5 CATEGORIAS (Receita):')\n",
    "    top_cat = batch_df.groupBy('category') \\\n",
    "        .agg(sum('revenue').alias('revenue')) \\\n",
    "        .orderBy(col('revenue').desc()) \\\n",
    "        .limit(5)\n",
    "    top_cat.show(truncate=False)\n",
    "    \n",
    "    # Distribui√ß√£o regional\n",
    "    print(f'üåé DISTRIBUI√á√ÉO POR REGI√ÉO:')\n",
    "    regions = batch_df.groupBy('region') \\\n",
    "        .agg(\n",
    "            count('*').alias('transactions'),\n",
    "            sum('revenue').alias('revenue')\n",
    "        ) \\\n",
    "        .orderBy(col('revenue').desc())\n",
    "    regions.show(truncate=False)\n",
    "    \n",
    "    # Totais acumulados\n",
    "    print(f'\\nüìà TOTAIS ACUMULADOS:')\n",
    "    print(f'   üì¶ Batches processados: {batch_stats[\"count\"]}')\n",
    "    print(f'   üìä Transa√ß√µes totais: {batch_stats[\"total_records\"]:,}')\n",
    "    print(f'   üíµ Receita total: ${batch_stats[\"total_revenue\"]:,.2f}')\n",
    "    \n",
    "    print('\\n' + '-'*100)\n",
    "    print(f'‚úÖ Batch #{batch_stats[\"count\"]} conclu√≠do!')\n",
    "    print('-'*100 + '\\n')\n",
    "\n",
    "print('‚úÖ Fun√ß√£o process_batch() definida!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b4154",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ EXECUTAR Streaming com foreachBatch\n",
    "\n",
    "**Execute esta c√©lula e observe o processamento arquivo por arquivo!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetar estat√≠sticas\n",
    "batch_stats = {'count': 0, 'total_records': 0, 'total_revenue': 0}\n",
    "\n",
    "print('üöÄ INICIANDO STRUCTURED STREAMING')\n",
    "print('='*100)\n",
    "print(f'üìÇ Source: {INPUT_PATH}')\n",
    "print(f'‚öôÔ∏è  maxFilesPerTrigger: 1')\n",
    "print(f'‚è±Ô∏è  Trigger: a cada 5 segundos')\n",
    "print(f'üîñ Checkpoint: {CHECKPOINT_PATH}')\n",
    "print('='*100)\n",
    "\n",
    "# Iniciar stream com foreachBatch\n",
    "query = enriched_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option('checkpointLocation', CHECKPOINT_PATH) \\\n",
    "    .start()\n",
    "\n",
    "print(f'\\n‚úÖ Stream ativo!')\n",
    "print(f'üÜî Query ID: {query.id}')\n",
    "print(f'üìä Status: {query.status}')\n",
    "print(f'\\nüí° Observe o processamento arquivo por arquivo abaixo...')\n",
    "print(f'‚è∏Ô∏è  Execute a pr√≥xima c√©lula para parar ap√≥s ~60 segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77151241",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Parar o Streaming\n",
    "\n",
    "**Aguarde ~60 segundos para processar todos os arquivos, ent√£o execute esta c√©lula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfcef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print('‚è≥ Aguardando 60 segundos para processar todos os arquivos...')\n",
    "time.sleep(60)\n",
    "\n",
    "query.stop()\n",
    "\n",
    "print('\\n' + '='*100)\n",
    "print('üõë STREAMING FINALIZADO')\n",
    "print('='*100)\n",
    "print(f'\\nüìä RESUMO FINAL:')\n",
    "print(f'   üì¶ Total de batches: {batch_stats[\"count\"]}')\n",
    "print(f'   üí≥ Total de transa√ß√µes: {batch_stats[\"total_records\"]:,}')\n",
    "print(f'   üí∞ Receita total: ${batch_stats[\"total_revenue\"]:,.2f}')\n",
    "print(f'   ‚úÖ Checkpoint salvo em: {CHECKPOINT_PATH}')\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa57ed",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ OP√á√ÉO 2: Gravar em Tabela Delta Lake\n",
    "\n",
    "**Alternativa: processar e gravar diretamente em Delta Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b027ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üöÄ Iniciando streaming com sa√≠da para Delta Lake...')\n",
    "print(f'üíæ Tabela: {OUTPUT_PATH}')\n",
    "print(f'üîñ Checkpoint: {CHECKPOINT_PATH}_delta\\n')\n",
    "\n",
    "# Streaming para Delta com agrega√ß√µes\n",
    "query_delta = aggregated_df.writeStream \\\n",
    "    .format('delta') \\\n",
    "    .outputMode('complete') \\\n",
    "    .option('checkpointLocation', f'{CHECKPOINT_PATH}_delta') \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start(OUTPUT_PATH)\n",
    "\n",
    "print(f'‚úÖ Stream Delta ativo! Query ID: {query_delta.id}')\n",
    "print(f'üí° Execute as pr√≥ximas c√©lulas ap√≥s ~60 segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "query_delta.stop()\n",
    "print('‚úÖ Stream Delta finalizado!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971172d",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Consultar Tabela Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06851837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler tabela Delta\n",
    "delta_df = spark.read.format('delta').load(OUTPUT_PATH)\n",
    "\n",
    "print(f'üìä Tabela Delta carregada!')\n",
    "print(f'üìà Total de registros: {delta_df.count():,}\\n')\n",
    "\n",
    "print('üèÜ TOP 10 - Categoria + Regi√£o (por Receita):')\n",
    "delta_df.orderBy(col('total_revenue').desc()).limit(10).show(truncate=False)\n",
    "\n",
    "# Totais gerais\n",
    "totals = delta_df.agg(\n",
    "    sum('total_transactions').alias('transactions'),\n",
    "    sum('total_revenue').alias('revenue'),\n",
    "    sum('unique_customers').alias('customers')\n",
    ").collect()[0]\n",
    "\n",
    "print(f'\\nüí∞ TOTAIS GERAIS:')\n",
    "print(f'   üí≥ Transa√ß√µes: {totals.transactions:,.0f}')\n",
    "print(f'   üíµ Receita: ${totals.revenue:,.2f}')\n",
    "print(f'   üë• Clientes: {totals.customers:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232909f",
   "metadata": {},
   "source": [
    "## üîü Criar Tabela Gerenciada no Catalog (Databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_DATABRICKS:\n",
    "    # Criar database\n",
    "    spark.sql('CREATE DATABASE IF NOT EXISTS streaming_demo')\n",
    "    \n",
    "    # Criar tabela gerenciada\n",
    "    delta_df.write \\\n",
    "        .format('delta') \\\n",
    "        .mode('overwrite') \\\n",
    "        .saveAsTable('streaming_demo.sales_summary')\n",
    "    \n",
    "    print('‚úÖ Tabela criada: streaming_demo.sales_summary')\n",
    "    print('\\nüìä Amostra da tabela:')\n",
    "    \n",
    "    spark.sql('SELECT * FROM streaming_demo.sales_summary LIMIT 10').show(truncate=False)\n",
    "    \n",
    "    print('\\nüí° Agora voc√™ pode consultar com SQL:')\n",
    "    print('   SELECT category, SUM(total_revenue) FROM streaming_demo.sales_summary GROUP BY category')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Tabelas gerenciadas dispon√≠veis apenas no Databricks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a334b03",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Limpeza de Recursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üßπ Limpando recursos...\\n')\n",
    "\n",
    "# Parar todos os streams ativos\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "    print(f'‚úì Stream {s.id} parado')\n",
    "\n",
    "# Remover arquivos (Databricks)\n",
    "if IS_DATABRICKS:\n",
    "    paths = [INPUT_PATH, OUTPUT_PATH, CHECKPOINT_PATH, f'{CHECKPOINT_PATH}_delta']\n",
    "    for path in paths:\n",
    "        try:\n",
    "            dbutils.fs.rm(path, True)\n",
    "            print(f'‚úì Removido: {path}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print('\\n‚úÖ Limpeza conclu√≠da!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248196e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Resumo do Notebook\n",
    "\n",
    "### O que foi demonstrado:\n",
    "1. ‚úÖ **Structured Streaming** com `maxFilesPerTrigger=1`\n",
    "2. ‚úÖ Processamento arquivo por arquivo vis√≠vel em tempo real\n",
    "3. ‚úÖ Agrega√ß√µes din√¢micas por categoria e regi√£o\n",
    "4. ‚úÖ Grava√ß√£o em **Delta Lake**\n",
    "5. ‚úÖ Cria√ß√£o de **tabelas gerenciadas** no Catalog\n",
    "6. ‚úÖ M√©tricas detalhadas: transa√ß√µes, receita, clientes, ticket m√©dio\n",
    "7. ‚úÖ Monitoramento de streams ativos\n",
    "\n",
    "### Conceitos-chave para ensinar:\n",
    "- **maxFilesPerTrigger**: controla quantos arquivos processar por trigger\n",
    "- **foreachBatch**: permite l√≥gica customizada para cada micro-batch\n",
    "- **checkpointLocation**: garante exactly-once processing e recupera√ß√£o de falhas\n",
    "- **Delta Lake**: formato ACID transacional para data lakes\n",
    "- **outputMode**: \n",
    "  - `append` - adiciona novos registros\n",
    "  - `complete` - reescreve resultado completo (usado com agrega√ß√µes)\n",
    "  - `update` - atualiza apenas registros modificados\n",
    "\n",
    "### Configura√ß√µes importantes:\n",
    "- **trigger**: define quando processar (processingTime, once, continuous)\n",
    "- **Schema inference**: sempre defina schema expl√≠cito em produ√ß√£o\n",
    "- **Watermarks**: para processar dados com atraso temporal\n",
    "- **Particionamento**: organize dados por data, regi√£o, categoria\n",
    "\n",
    "### Para produ√ß√£o:\n",
    "- Ajustar `maxFilesPerTrigger` conforme volume de dados\n",
    "- Configurar triggers apropriados (n√£o usar 5 segundos em prod)\n",
    "- Implementar watermarks para dados atrasados\n",
    "- Adicionar tratamento de erros robusto\n",
    "- Configurar alertas e monitoramento (Spark UI, Ganglia, Grafana)\n",
    "- Usar particionamento eficiente no Delta Lake\n",
    "- Testar recupera√ß√£o de falhas (checkpoint)\n",
    "\n",
    "### Pr√≥ximos passos para seus alunos:\n",
    "1. Modificar agrega√ß√µes (adicionar mais m√©tricas)\n",
    "2. Experimentar diferentes outputModes\n",
    "3. Adicionar watermarks para janelas temporais\n",
    "4. Testar com volumes maiores de dados\n",
    "5. Integrar com fontes reais (S3, Azure Blob, GCS)\n",
    "\n",
    "**üéì Perfeito para ensinar os fundamentos de Structured Streaming!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
